{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMwxfkAdM0fxWxPv5Bierpw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"gqOCYD6xKxR6"},"outputs":[],"source":["!pip install jedi stable-baselines3[extra] pyglet"]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","import gym\n","import numpy as np\n","import matplotlib.pyplot as plt"],"metadata":{"id":"8UKj4c_VNmrk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Actor(nn.Module):\n","  def __init__(self, state_dim, action_dim, learning_rate):\n","    super().__init__()\n","    self.fc1 = nn.Linear(state_dim, 64)\n","    self.fc2 = nn.Linear(64, 32)\n","    self.fc3 = nn.Linear(32, 16)\n","    self.mean = nn.Linear(16, action_dim)\n","    self.log_std = nn.Linear(16, action_dim)\n","    self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n","\n","  def forward(self, state):\n","    x = torch.relu(self.fc1(state))\n","    x = torch.relu(self.fc2(x))\n","    x = torch.relu(self.fc3(x))\n","    mean = 2 * torch.tanh(self.mean(x)) # pendulum: action space in [-2, 2]\n","    std = torch.exp(self.log_std(x))\n","    return mean, std"],"metadata":{"id":"fjDviK1IN7Is"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Critic(nn.Module):\n","  def __init__(self, state_dim, learning_rate):\n","    super().__init__()\n","    self.fc1 = nn.Linear(state_dim, 64)\n","    self.fc2 = nn.Linear(64, 32)\n","    self.fc3 = nn.Linear(32, 16)\n","    self.value = nn.Linear(16, 1)\n","    self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n","\n","  def forward(self, state):\n","    x = torch.relu(self.fc1(state))\n","    x = torch.relu(self.fc2(x))\n","    x = torch.relu(self.fc3(x))\n","    return self.value(x)"],"metadata":{"id":"BoASdr1FPVhS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class PPOagent(object):\n","  def __init__(self, env):\n","    self.env = env\n","    self.state_dim = env.observation_space.shape[0]\n","    self.action_dim = env.action_space.shape[0]\n","\n","    # Hyperparameters\n","    self.GAMMA = 0.99            # discount factor\n","    self.LAMBDA = 0.95           # for GAE\n","    self.EPSILON = 0.2           # for clipping the ratio\n","    self.LR_ACTOR = 2e-4\n","    self.LR_CRITIC = 2e-3\n","    self.NUM_STEPS = 1024        # how long will it follow the trajectory\n","    self.EPOCHS = 50             # how many times will it use the old policy to learn\n","    self.NUM_EPISODES = 1000\n","\n","    self.actor = Actor(self.state_dim, self.action_dim, self.LR_ACTOR)\n","    self.critic = Critic(self.state_dim, self.LR_CRITIC)\n","\n","    self.ep_rewards = []\n","\n","  def GAE(self, rewards, values, next_value, done):\n","    gaes = torch.zeros_like(torch.tensor(rewards))\n","    gae_sum = 0\n","    if done:\n","      next_value = 0\n","    for i in reversed(range(0, len(rewards))):\n","      delta = rewards[i] + self.GAMMA * next_value - values[i]\n","      gae_sum = delta + self.GAMMA * self.LAMBDA * gae_sum\n","      gaes[i] = gae_sum\n","      next_value = values[i]\n","    return gaes[0]\n","\n","  def update_actor(self, old_log_probs, states, actions, gaes):\n","    mean, std = self.actor(torch.tensor(states))\n","    dist = torch.distributions.Normal(mean, torch.clamp(std, 1e-2, 1.0))\n","    log_probs = dist.log_prob(actions)\n","    ratios = torch.exp(log_probs - old_log_probs)\n","    clipped_ratios = torch.clamp(ratios, 1.0 - self.EPSILON, 1.0 + self.EPSILON)\n","    actor_loss = -torch.mean(torch.min(ratios * gaes, clipped_ratios * gaes), dim=0)\n","    self.actor.optimizer.zero_grad()\n","    actor_loss.backward()\n","    self.actor.optimizer.step()\n","\n","  def update_critic(self, gaes):\n","    critic_loss = torch.mean(torch.square(gaes))\n","    self.critic.optimizer.zero_grad()\n","    critic_loss.backward()\n","    self.critic.optimizer.step()\n","\n","  def sample_action(self, state):\n","    mean, std = self.actor(torch.tensor(state))\n","    dist = torch.distributions.Normal(mean, torch.clamp(std, 1e-2, 1.0))\n","    action = torch.clamp(dist.sample(), -2, 2) # pendulum: action space in [-2, 2]\n","    log_prob = dist.log_prob(action)\n","    return action, log_prob\n","\n","  def train(self):\n","    for ep in range(self.NUM_EPISODES):\n","      states, actions, rewards, log_probs = [], [], [], []\n","      done = False\n","      state = self.env.reset()\n","      for step in range(self.NUM_STEPS):\n","        action, log_prob = self.sample_action(state)\n","        next_state, reward, done, _ = self.env.step(action.numpy())\n","        reward = (reward + 8) / 8 # pendulum: return in [-16.xx, 0]\n","\n","        states.append(state)\n","        actions.append(action)\n","        rewards.append(reward)\n","        log_probs.append(log_prob)\n","        state = next_state\n","\n","        if done: \n","          break\n","\n","      old_log_probs = torch.stack(log_probs).detach()\n","      actions = torch.stack(actions)\n","\n","      for _ in range(self.EPOCHS):\n","        values = self.critic(torch.tensor(states))\n","        next_value = self.critic(torch.tensor(next_state))\n","        gaes = self.GAE(rewards, values, next_value, done)\n","\n","        self.update_actor(old_log_probs, states, actions, gaes.detach())\n","        self.update_critic(gaes)\n","\n","      ep_reward = np.mean(rewards)\n","      print(f\"Episode: {ep+1}, Mean Reward: {ep_reward}\")\n","      self.ep_rewards.append(ep_reward)\n","\n","  def plot_ep_rewards(self):\n","    plt.plot(self.ep_rewards)\n","    plt.show()"],"metadata":{"id":"_7dpv_qBP4qP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["env = gym.make(\"Pendulum-v1\")\n","agent = PPOagent(env) \n","\n","agent.train()\n","agent.plot_ep_rewards()"],"metadata":{"id":"BLDYmH16uktj"},"execution_count":null,"outputs":[]}]}